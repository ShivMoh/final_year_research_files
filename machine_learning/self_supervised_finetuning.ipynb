{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hf_args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhf_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelArguments, DataArguments, TrainingArguments\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     Trainer,\n\u001b[1;32m      9\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[1;32m     10\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m     11\u001b[0m     AutoTokenizer\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     prepare_model_for_kbit_training,\n\u001b[1;32m     15\u001b[0m     LoraConfig, \n\u001b[1;32m     16\u001b[0m     get_peft_model\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hf_args'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "import transformers\n",
    "from hf_args import ModelArguments, DataArguments, TrainingArguments\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig, \n",
    "    get_peft_model\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from utils import print_gpu_utilization\n",
    "from utils.flash_attention_patch import replace_attn_with_flash_attn, forward\n",
    "from utils.save_peft_callback import SavePeftModelCallback\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def find_all_linear_names(args, model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "def main():\n",
    "    hfparser = transformers.HfArgumentParser((\n",
    "        ModelArguments, DataArguments, TrainingArguments\n",
    "    ))\n",
    "    (\n",
    "        model_args, \n",
    "        data_args, \n",
    "        training_args, \n",
    "        extra_args\n",
    "    ) = hfparser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "    \n",
    "    args = argparse.Namespace(\n",
    "        **vars(model_args), **vars(data_args), **vars(training_args)\n",
    "    )\n",
    "    \n",
    "    # replace attention\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:\n",
    "        print(\"Using flash attention\")\n",
    "        replace_attn_with_flash_attn()\n",
    "        use_flash_attention = True\n",
    "    \n",
    "    # get the model\n",
    "    print(f'loading base model {args.model_name_or_path}...')        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        cache_dir=args.cache_dir,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "        # load_in_4bit=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map='auto'  # ToDo: set to none for deepspeed\n",
    "    )\n",
    "    \n",
    "    # load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        cache_dir=args.cache_dir,\n",
    "        use_fast=False,\n",
    "        tokenizer_type='llama' if 'llama' in args.model_name_or_path else None, # Needed for HF name change\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "    )\n",
    "    \n",
    "    model.resize_token_embeddings(len(tokenizer))  # ToDo: can this be moved to creation of the custom tokenizer?\n",
    "    \n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # loads the LoraConfigs or checkpoints\n",
    "    print(f'adding LoRA modules...')\n",
    "    modules = find_all_linear_names(args, model)\n",
    "    config = LoraConfig(\n",
    "        r = args.lora_r,\n",
    "        lora_alpha = args.lora_alpha,\n",
    "        target_modules = modules,\n",
    "        lora_dropout = args.lora_dropout,\n",
    "        bias = \"none\",\n",
    "        task_type = \"CAUSAL_LM\",\n",
    "        modules_to_save=['embed_tokens', 'lm_head'],  # Test to see if it works and helps the embedding\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "    # load the dataset\n",
    "    ds = load_from_disk(args.dataset)\n",
    "    if training_args.do_eval:\n",
    "        ds = ds.train_test_split(test_size=0.1)\n",
    "    ds = ds.with_format('torch')\n",
    "    \n",
    "    training_args.bf16 = True\n",
    "    \n",
    "    # do the training\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        train_dataset = ds['train'] if training_args.do_eval else ds,\n",
    "        eval_dataset = ds['test'] if training_args.do_eval else None,\n",
    "        args = training_args,        \n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    trainer.add_callback(SavePeftModelCallback)\n",
    "    \n",
    "    print(f'beginning training...')\n",
    "    result = trainer.train()\n",
    "    trainer.save_state()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_sort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
